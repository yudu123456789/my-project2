# my-project2
选项一：大数据大作业（健康码红码模拟）README
大数据实验报告：健康码红码生成模拟系统
1. 项目背景

在新冠抗疫期间，通过手机漫游信息（基站定位）识别与感染者轨迹有交集的人员，是精准防控的关键。本项目基于 Spark 分布式计算框架，实现海量漫游数据的时空碰撞分析。

2. 核心算法逻辑

数据配对：利用窗口函数对原始基站信令进行排序，将“进站”与“离站”事件配对，构建完整的轨迹区间 [Entry_Time, Exit_Time]。

时空碰撞：

空间维度：潜在接触者与感染者处于同一 Base_Station_ID。

时间维度：满足 潜在进站 <= 感染者离站 且 潜在离站 >= 感染者进站。

结果去重：筛选出所有符合条件的手机号码并排除感染者本人。

3. 技术栈

编程语言：Scala

计算框架：Spark SQL / DataFrame API

存储系统：HDFS

资源管理：Spark on YARN

4. 运行环境与配置

集群规模：15个从节点（Slave Nodes）

核心参数：

--num-executors: 20

--executor-memory: 16G

--executor-cores: 2

--driver-memory: 16G

垃圾回收: G1 GC

5. 使用说明

将 cdinfo.txt 和 Infected.txt 上传至 HDFS 指定目录。

使用 sbt 打包 Scala 代码为 jar 包。

通过 spark-submit 提交任务至集群运行。

在输出路径查看生成的红码手机号列表。

选项二：数学实践（120G 海量数据外部排序）README
数学实践项目：海量数据外部排序与采样系统
1. 项目简介

针对总大小为 120GB、单条记录长度为 15 字符的随机字符串数据集进行排序。本项目实现了经典的**外部归并排序（External Merge Sort）**算法，旨在解决内存远小于数据量时的海量数据处理问题。

2. 功能模块

外部排序 (External Sort)：将 120G 大文件切分为多个符合内存大小的 Chunk，并在内存中完成快速排序后存回磁盘。

多路归并 (Merge Sort)：通过多指针机制，同时读取多个有序小文件，利用 min() 函数选出全局最小值，输出为完整有序大文件。

生成器迭代读取：使用 Python yield 机制，确保在采样和打印阶段仅占用极低内存。

等差采样输出：按照公式 i % interval == starting_index % interval（其中间隔受组号影响）进行结果验证。

3. 实现细节

语言：Python 3.x

核心逻辑：

分块排序：将文件按 1GB/Chunk 进行内排。

归并合并：实现 24 路或更多路数的外部归并。

结果采样：每隔 100,000+ 行（受组号偏移影响）提取一条数据。

4. 快速开始

数据准备：确保输入文件 data.txt 已就绪。

执行排序：运行主程序，系统将自动进行分块排序和多路归并。

输出验证：采样结果将自动保存至指定路径（如 E:/1234/10/result.txt），用于正确性校验和速度评分。
